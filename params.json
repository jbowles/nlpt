{"name":"Nlpt","tagline":"Natural Language Processing Toolkit written in Go","body":"# Natural Language Processing Toolkit\r\nWritten in Go.\r\n\r\nIf you wanna see where all the work is happening, go to [exp branch](https://github.com/jbowles/nlpt/tree/exp).\r\n\r\nThis is the tested, stable, and production ready brnach of a research project to write natural language processing tools in Go. NLPT is built up from multiple sub-packages (each separately accessible).\r\n\r\nGet it:\r\n* `go get github.com/jbowles/nlpt` \r\n  * (or update: `go get -u github.com/jbowles/nlpt`)\r\n\r\nFunctionality is separated into sub packages, which are usable outside the scope of the main NLPT project. Naming of each subpackage will be consistent as per the `first 3 letter prefix` + `subpackage name`. For example: tokenizer = `nlptokenizer`, stemmer = `nlpstemmer`, tagger = `nlptagger`.\r\nGet a subpackage:\r\n* `go get github.com/jbowles/nlpt/nlptokenizer` \r\n  * (or update: `go get -u github.com/jbowles/nlpt/nlptokenizer`)\r\n\r\nThanks to the Go Berlin users group for letting me ripoff their gopher image!\r\n\r\n![Alt text](https://github.com/jbowles/nlpt/raw/exp/nlpt.jpg \"Natural Language Processing Toolkit in Go\")\r\n\r\n## Branches\r\n* `exp`\r\n  * Low-level development and general messiness\r\n* `stable`\r\n  * Testing, performance, run standard data sets, P&R (precision and recall) where appropriate\r\n\r\nDevelopment workflow == `exp` -> `stable` -> `master`\r\n\r\n## Criteria each sub-package:\r\n* `Stability` (Experimental, Stable, Production) to determine whether the API is production ready. \r\n* `Volatility` (Radical, Mild, Stable) to determine whether the API is likely to change.\r\n* `Test` (Nil, Some, Stable) to signal range of coverage for tests over the API.\r\n* `Examples` link to external repo with more documentation and examples.\r\n\r\n## General\r\nNLPT broadly supports minimal functionality for text in following language sets:\r\n\r\n* Text in Roman alphabet with diacritics (English, Spanish, French, German, etc...)\r\n* Text in Cyrillic alphabet (Russian, Belarusian, Ukrainian, Rusyn, Serbian, Bulgarin, Macedonian, Chechen, and other Slavic langauges.\r\n* Text with Greek alphabet\r\n* **Support for Arabic and Mandarin are coming in late 2014**\r\n\r\n## Tokenizer\r\n\r\n    Stability:  3   - Stable\r\n    Volatility: 3   - Stable\r\n    Tests:      3   - Stable\r\nExamples:   []()\r\n\r\n### TODO\r\n* Support for Arabic and Mandarin are coming, though probably not until late 2014.\r\n* Eventually move to a more probabilistic model.\r\n\r\n### Description\r\nTokenizer it leverages the Go Rune Type (`int32` aliases for Unicode). Basically, you can **build custom unicode alphabets** that are used for pattern matching (instead of regular expressions). General goals:\r\n\r\n1. Broader spectrum of Unicode characaters used across ever expanding and changing media\r\n1. Special or nonstandard characters used in software application logs\r\n1. The rise of new languages on the web, and the velocity of disappearing endangered langauges\r\n\r\n### Sources\r\n* Unicode code points table for Go Runes\r\n  * [UTF8 Character Table (in decimal)](http://www.utf8-chartable.de/unicode-utf8-table.pl?utf8=dec)\r\n\r\n\r\n## TF-IDF: Term Frequency-Inverse Document Frequency\r\nThe Tf-Idf stuff is not done, I've just been playing with different ways of doing it. There is not a full model finished yet and so the first implementation is not complete.\r\n\r\n    Stability:  0   - Not Started\r\n    Volatility: 0   - Not Started\r\n    Tests:      0   - Not Started\r\nExamples:  []()\r\n\r\n## Stemmer\r\n\r\n    Stability:  0   - Not Started\r\n    Volatility: 0   - Not Started\r\n    Tests:      0   - Not Started\r\nExamples:  []()\r\n\r\n## POS: Part of Speech Tagger\r\n\r\n    Stability:  0   - Not Started\r\n    Volatility: 0   - Not Started\r\n    Tests:      0   - Not Started\r\nExamples:  []()\r\n\r\n# Resources\r\n\r\n## Tokenizer\r\n* [Xerox Tokenizer Service](http://open.xerox.com/Services/fst-nlp-tools/Consume/175)\r\n\r\n## Stemming\r\n* [Xerox Morphological Analysis](http://open.xerox.com/Services/fst-nlp-tools/Pages/morphology) \r\n\r\n## Part of Speech Tags\r\n* [Xerox POS Tagger Service](http://open.xerox.com/Services/fst-nlp-tools/Consume/178)\r\n* [Xerox POS Tagset Standard](http://open.xerox.com/Services/fst-nlp-tools/Pages/English%20Part-of-Speech%20Tagset)\r\n\r\n## Tf-Idf\r\nSee the IDF entry in the [Information Retrieval](http://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html) book provided by Stanford and authors for detail.\r\n\r\n## Unicode\r\n* [Lorem Ipsum generator for multiple languages](http://generator.lorem-ipsum.info/)\r\n  * Includes Arabic, Mandarin, Hebrew, Cyrillic, and others\r\n* [Library of Congress Standards](http://www.loc.gov/standards/)\r\n* [ISO 693-2 Standard for Natural Language Codes and Names](http://www.loc.gov/standards/iso639-2/php/code_list.php)\r\n* [ISO 639-5 Standard for Natural Language Families and Names](http://www.loc.gov/standards/iso639-5/id.php)\r\n\r\n## NLP APIs for result comaprison testing\r\nOne reason for knowing about NLP services is they can be used for testing and comparing results. When more sub-packages become available tests will be written against these APIs to compare results.\r\n\r\n#### Xerox\r\n* [Xerox Linguistic Tools](http://open.xerox.com/Services/fst-nlp-tools/Pages/API%20Docs)\r\n  * \"Finite State Technology Tools for Natural Language Processing\"\r\n\r\n  ```go\r\n  /*\r\n     ** DESCRIPTION FROM XEROX **\r\n  These tools, called xfst, twolc, and lexc, are used in many linguistic applications such as morphological analysis, tokenisation, and shallow parsing of a wide variety of natural languages. The finite state tools here are built on top of a software library that provides algorithms to create automata from regular expressions and equivalent formalisms and contains both classical operations, such as union and composition, and new algorithms such as replacement and local sequentialisation.\r\n\r\n  Finite-state linguistic resources are used in a series of applications and prototypes that range from OCR to terminology extraction, comprehension assistants, digital libraries and authoring and translation systems.\r\n\r\n  The components provided here are:\r\n\r\n  Tokenization\r\n  Morphology\r\n  Part of Speech Disambiguation (Tagging) \r\n  */\r\n  ```\r\n\r\n#### Alchemy\r\n* [AlchemyApi](http://www.alchemyapi.com/)\r\n  * \"... cloud-based and on-premise text analysis infrastructure\"\r\n\r\n  ```go\r\n  /*\r\n  ** DESCRIPTION FROM ALCHEMY **\r\n  AlchemyAPI uses natural language processing technology and machine learning algorithms to extract semantic meta-data from content, such as information on people, places, companies, topics, facts, relationships, authors, and languages.\r\n\r\n  API endpoints are provided for performing content analysis on Internet-accessible web pages, posted HTML or text content.\r\n  */\r\n  ```\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}